defaults:
- _self_
- motion: skills.yaml
- environment: humanoid.yaml
- network: amp.yaml
- hydra: default.yaml
- paths: default.yaml
- extras: default.yaml

# - metrics: default.yaml

algo:
  name: amp

model:
  name: amp

seed: 0
train: True

# resume
load_checkpoint: False
checkpoint: None # agent.restore(self.load_path) # [ ]

config:
  task_name: &task_name Humanoid # task name, used in output folder
  name: *task_name
  env_name: rlgpu

  use_gpu: True
  multi_gpu: False
  torch_deterministic: False

  headless: True
  physics_engine: "physx" # or "flex"
  num_threads: 0 # Number of cores used by PhysX
  subscenes: &subscenes 0 # Number of PhysX subscenes to simulate in parallel
  slices: *subscenes # Number of client threads that process env slices, default to be subscenes

  train_dir: ${paths.root_dir}/logs/
  full_experiment_name: ${config.task_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}

  mixed_precision: False
  normalize_input: True
  normalize_value: True
  reward_shaper:
    scale_value: 1
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 2e-5
  lr_schedule: constant
  score_to_win: 20000
  max_epochs: 10000
  save_best_after: 50
  save_frequency: 50
  print_stats: True
  grad_norm: 1.0
  entropy_coef: 0.0
  truncate_grads: False
  ppo: True
  e_clip: 0.2
  horizon_length: 32
  minibatch_size: 16384
  mini_epochs: 6
  critic_coef: 5
  clip_value: False
  seq_len: 4
  bounds_loss_coef: 10
  amp_obs_demo_buffer_size: 200
  amp_replay_buffer_size: 200
  amp_replay_keep_prob: 0.01
  amp_batch_size: 512
  amp_minibatch_size: 4096
  disc_coef: 5
  disc_logit_reg: 0.01
  disc_grad_penalty: 5
  disc_reward_scale: 2
  disc_weight_decay: 0.0001
  normalize_amp_input: True
  enable_eps_greedy: False

  task_reward_w: 0.5
  disc_reward_w: 0.5

  # log_image
